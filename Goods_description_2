import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.evaluation.MulticlassMetrics
import org.apache.spark.mllib.feature.{Tokenizer,HashingTF}
import org.apache.spark.rdd.PairRDDFunctions


val rawData = sc.textFile("/home/manish/Desktop/data.csv")

---------------------------
//Removing Punctuation, words <=2,  \s* to remove redundant spaces

val rawData1 = rawData.map(x => x.replaceAll("""([\p{Punct}&&[^|$]]|\b\p{IsLetter}{1,2}\b)\s*""", ""))


// To convert into DataFrame : http://spark.apache.org/docs/1.5.2/ml-features.html#tokenizer

org.apache.spark.sql.DataFrame

val dataDF = rawData1.toDF("label", "sentence")

// Tokenize RDD

val tokenizer = new Tokenizer().setInputCol("sentence").setOutputCol("words")
val regexTokenizer = new RegexTokenizer()
  .setInputCol("sentence")
  .setOutputCol("words")
  .setPattern("\\W")

val tokenized = tokenizer.transform(rawData1)
tokenized.select("words", "label").take(3).foreach(println)





// Removing stop words

val stopWords = sc.parallelize(List ("a","about","above","above","across","after","afterwards","again","against","all","almost","alone","along","already","also","although","always","am","among","amongst","amoungst","amount","an","and","another","any","anyhow","anyone","anything","anyway","anywhere","are","around","as","at","back","be","became","because","become","becomes","becoming","been","before","beforehand","behind","being","below","beside","besides","between","beyond","bill","both","bottom","but","by","call","can","cannot","cant","co","con","could","couldnt","cry","de","describe","detail","do","done","down","due","during","each","eg","eight","either","eleven","else","elsewhere","empty","enough","etc","even","ever","every","everyone","everything","everywhere","except","few","fifteen","fify","fill","find","fire","first","five","for","former","formerly","forty","found","four","from","front","full","further","get","give","go","had","has","hasnt","have","he","hence","her","here","hereafter","hereby","herein","hereupon","hers","herself","him","himself","his","how","however","hundred","ie","if","in","inc","indeed","interest","into","is","it","its","itself","keep","last","latter","latterly","least","less","ltd","made","many","may","me","meanwhile","might","mill","mine","more","moreover","most","mostly","move","much","must","my","myself","name","namely","neither","never","nevertheless","next","nine","no","nobody","none","noone","nor","not","nothing","now","nowhere","of","off","often","on","once","one","only","onto","or","other","others","otherwise","our","ours","ourselves","out","over","own","part","per","perhaps","please","put","rather","re","same","see","seem","seemed","seeming","seems","serious","several","she","should","show","side","since","sincere","six","sixty","so","some","somehow","someone","something","sometime","sometimes","somewhere","still","such","system","take","ten","than","that","the","their","them","themselves","then","thence","there","thereafter","thereby","therefore","therein","thereupon","these","they","thickv","thin","third","this","those","though","three","through","throughout","thru","thus","to","together","too","top","toward","towards","twelve","twenty","two","un","under","until","up","upon","us","very","via","was","we","well","were","what","whatever","when","whence","whenever","where","whereafter","whereas","whereby","wherein","whereupon","wherever","whether","which","while","whither","who","whoever","whole","whom","whose","why","will","with","within","without","would","yet","you","your","yours","yourself","yourselves","the"))

val proData = rawData1.filter(!stopWords.contains(_))


def processLine(s: String, stopWords: Set[String]): List[String] = {
      s.toLowerCase()
      .replaceAll("""([\p{Punct}&&[^|$]]|\b\p{IsLetter}{1,2}\b)\s*""", "")
      .replaceAll("""\.""", " .")
      .split("\\s+")
      .filter(!stopWords.contains(_))
      .toList
  }



----------------------------

val htf = new HashingTF(1000)

val parsedData = rawData1.map { line =>
val values = (line.split("|").toSeq)
val featureVector = htf.transform(values(1).split(" "))
val label = values(0).toDouble
LabeledPoint(label, featureVector)
}

//parsedData.foreach(println)

(1.0,(1000,[48],[1.0]))
(3.0,(1000,[49],[1.0]))
(1.0,(1000,[48],[1.0]))
(3.0,(1000,[49],[1.0]))
(1.0,(1000,[48],[1.0]))
(3.0,(1000,[49],[1.0]))
(1.0,(1000,[48],[1.0]))
(3.0,(1000,[49],[1.0]))
(1.0,(1000,[48],[1.0]))

val splits = parsedData.randomSplit(Array(0.8, 0.2), seed = 11L)
val training = splits(0)
val test = splits(1)

val model = NaiveBayes.train(training, lambda = 2.0, modelType = "multinomial")


val predictionAndLabels = test.map { point => 
  val score = model.predict(point.features)
  (score, point.label)
}
    

val metrics = new MulticlassMetrics(predictionAndLabels)
    

metrics.labels.foreach( l => println(metrics.fMeasure(l)))
	
val trainErr = predictionAndLabels.filter(r => r._1 != r._2).count.toDouble / training.count
println("Training Error = " + trainErr)	



val predictionAndLabel = test.map(p => (model.predict(p.features), p.label))
val accuracy = 1.0 * predictionAndLabel.filter(x => x._1 == x._2).count() / test.count()


val testData1 = htf.transform("mediation")
val predictionAndLabels1 = model.predict(testData1)

val testData1 = htf.transform("lost property return")
val predictionAndLabels1 = model.predict(testData1)

val testData1 = htf.transform("litigation services")
val predictionAndLabels1 = model.predict(testData1)

